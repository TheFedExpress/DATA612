{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vicous Cycle of Recommender Systems\n",
    "\n",
    "Many forms of supervised learning, particularly trees and SVMs, are about dividing the feature space into sections with the goal of making a generalization about those sections.  When the subject of the data is people, this can become problematic. The cost of false positives and false negatives can be high, for instance assuming a subsection of the population has been to prison when they have not, as the algorithm is compounding societal problems.  The method of Hardt, Price, and Srebro, where incorrect classification is given an additional cost, but is not a panacea.  Even in cases where the machine is correct, the feeling that they are being singled out can have a negative impact on the subject/user, both on their well-being and their continued use of the product.  This negative externality is difficult to capture in a purely utilitarian method, as Hardt, Price, and Srebro attempt.  I am of the belief that both the ethical and profit-motivated best practice is to not use certain demographic information to drive recommender systems.\n",
    "\n",
    "Like overfitting bleeding over into the test set, real world biases can seep onto recommendation algorithms.  The example of STEM fields and females is popular in recommender literature so I'll use this for our example.  I should also note that, while it is my guess that these assumptions are correct, they are merely for illustration purposes.  Assume that females are encouraged towards STEM education at lower rates than males.  It would stand to reason that they would pursue STEM education at lower rates and purchase fewer STEM-related books on Amazon.  Both the collaborative filtering piece, through the purchases of other females, and demographic data piece of Amazon's recommender will pick up on this and show females  few books on coding and sciences.  Because we know that customers make many of their purchases as a <a href = \"http://fortune.com/2012/07/30/amazons-recommendation-secret/\"> result of this feature</a> , this will expand the gap in STEM interest between males and females.  \n",
    "\n",
    "In the context of supervised learning for recommender systems, the not all of the model inputs can be controlled.  In linear regression, you are in full control of all possible input variables and interaction terms, even in penalized least squares.  The same cannot be said for collaborative filterting.  In user-user filtering, it wouldn't be feasible to catalog all users and know which may carry ethical pitfalls.  With matrix factorization, latent dimensions have some interpretability as linear combinations of items.  However, manually pruning would be time-intensive at best and would not scale well as dimensionality of the user-item matrix and latent features increase.  This is where the utiliatrian approach should be applied, with a cost given to discrimination-related errors.  Modern recommender systems are far more complex than simple collaborative filtering algorithms so the caution must be excercised with the additional inputs.  It would be quite reasonable to use deep learning as a feature transformer for the initial model layers and supplimenting that with ethically constructed demographic data for input into a penalized least squares regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://pdfs.semanticscholar.org/e82b/fb5671e0af4084f666d7048bdadc2dcbd8b1.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
